{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCB910SoaM9FY92AYkHQ+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/generative-ai/blob/master/rag/rag-1/rag_routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E26By-JRgmO8",
        "outputId": "825ec37b-54fd-41b5-d023-7544e6cac581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "print('hello world')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enviornment**"
      ],
      "metadata": {
        "id": "D8Wf0z9wRDM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube google-generativeai langchain_google_genai langchain_community"
      ],
      "metadata": {
        "id": "AekDM0f7Q_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Setup Keys For GOOGLE_API_KEY AND LANGSMITH"
      ],
      "metadata": {
        "id": "tzaWEi7vRMJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning value to variable\n",
        "GOOGLE_API_KEY=''\n",
        "LANGCHAIN_API_KEY = ''\n"
      ],
      "metadata": {
        "id": "dvFOprifTZNj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports os, google Gemini EMbedding and Chat Classes with FAISS vectorstores"
      ],
      "metadata": {
        "id": "uoqtF2Z2N8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "ALaWrOJvc3Qe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup langsmith for tracing"
      ],
      "metadata": {
        "id": "6Y_EPIyLOH0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "NSYmU7-pb0wM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Logical and Semantic routing]\n",
        "\n",
        "Use function-calling for classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "K_wDozkdzHPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing import Literal\n",
        "\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# # Data model\n",
        "# class RouteQuery(BaseModel):\n",
        "#     \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "#     datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "#         ...,\n",
        "#         description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "#     )\n",
        "\n",
        "# # LLM with function call\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3) # Try using gemini-pro-vision\n",
        "# structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "\n",
        "\n",
        "# # Prompt\n",
        "# # Prompt\n",
        "# system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "# Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
        "\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", system),\n",
        "#         (\"human\", \"{question}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Define router\n",
        "# router = prompt | structured_llm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3) # Try using gemini-pro-vision\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source by selecting one of the following options:\n",
        "\n",
        "python_docs\n",
        "js_docs\n",
        "golang_docs\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm\n",
        "print(router)\n"
      ],
      "metadata": {
        "id": "vJf7rL1CzJWR",
        "outputId": "1876b53a-404e-46a8-e2ca-45d1559245b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first=ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at routing a user question to the appropriate data source.\\n\\nBased on the programming language the question is referring to, route it to the relevant data source by selecting one of the following options:\\n\\npython_docs\\njs_docs\\ngolang_docs \\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]) middle=[RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.3, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x77ff35042950>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x77ff3506fc40>, default_metadata=()), kwargs={'tools': [{'function_declarations': [{'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'type_': 6, 'properties': {'datasource': {'type_': 1, 'description': 'Given a user question choose which datasource would be most relevant for answering their question', 'format_': '', 'nullable': False, 'enum': [], 'properties': {}, 'required': []}}, 'required': ['datasource'], 'format_': '', 'description': '', 'nullable': False, 'enum': []}}]}], 'tool_config': None})] last=PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "CFuaSefD0Y_i",
        "outputId": "10829cbc-5836-4ea7-a92a-0fbac6fe312e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='python_docs'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "yvwfllXe3Dm8",
        "outputId": "20091a7d-5029-45f8-d1b4-22c8762b258d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RouteQuery(datasource='python_docs')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.datasource\n"
      ],
      "metadata": {
        "id": "_asmcBJA3KGR",
        "outputId": "c53c217e-d4c5-4c93-8cb0-0c0f9d6f123f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'python_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"golang_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)\n"
      ],
      "metadata": {
        "id": "pnH9IHgL3VRI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\": question})\n"
      ],
      "metadata": {
        "id": "pRVsZO0i3YV4",
        "outputId": "e143b3cd-7846-4423-9041-60264f6e4221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chain for python_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}