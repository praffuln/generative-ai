{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/langchain/blob/master/langchain_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhAkJZxtp2yq",
        "outputId": "0838e52d-17e3-45e1-c497-ab2933258428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.335)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.64)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openAI in /usr/local/lib/python3.10/dist-packages (1.2.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openAI) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openAI) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openAI) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openAI) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openAI) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openAI) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openAI) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openAI) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openAI) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openAI) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openAI) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openAI) (0.14.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.7.22)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32004 sha256=a22230db64b5c101648926cc11f058466831b20349317f6e328eb865a34d39a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openAI\n",
        "!pip install wikipedia\n",
        "!pip install huggingface_hub\n",
        "!pip install google-search-results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***api keys***"
      ],
      "metadata": {
        "id": "jaqBcy983xoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "follow credentials.txt !! not commited in git branch."
      ],
      "metadata": {
        "id": "QJYRpl0f3vgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'follow credentials.txt'\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"follow credentials.txt\"\n",
        "os.environ['SERPAPI_API_KEY'] = 'follow credentials.txt'"
      ],
      "metadata": {
        "id": "w39nvEq931B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aHpdMKPwwx7",
        "outputId": "00b9128b-1717-4ac7-8efb-e7a4158d1a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "January 1976\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "#llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0})\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xxl\",\n",
        "    model_kwargs={\"temperature\": 0.5}\n",
        ")\n",
        "\n",
        "question = \"When was apple founded?\"\n",
        "\n",
        "print(llm(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg5175dHIuqE"
      },
      "source": [
        "PROMPTTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hSV0wWZoI0V0",
        "outputId": "f58e0f82-1977-445b-a9e9-5e5b660bfb05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When was apple founded? '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "template = \"When was {company} founded? \"\n",
        "prompt_template_company = PromptTemplate(input_variables =[\"company\"], template = template)\n",
        "prompt_template_company.format(company=\"apple\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK61nudoVQbY"
      },
      "source": [
        "**# Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B7XRCjvLJZg9",
        "outputId": "0f50f454-926a-4b88-b619-2148700aa2be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1890'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm = llm, prompt = prompt_template_company)\n",
        "chain.run(\"ibm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA53o7t4a_yr"
      },
      "source": [
        "Sequencial *chain*  \n",
        "\n",
        "SimpleSequentialChain =======\n",
        "\n",
        " Input(India restaurent) -->(restaurent name) --> Output(Maharaja) ---> (GIVE ME food menu items) ---> lassi, tandoori, tea, samosa\n",
        "\n",
        " It just give only one output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4vHYMtnrOqN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NAR9wA3bCND"
      },
      "outputs": [],
      "source": [
        "cuisine_name_template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        "cuisine_name_prompt = PromptTemplate(template=cuisine_name_template, input_variables= [\"cuisine\"])\n",
        "cuisine_name_chain = LLMChain(llm=llm, prompt=cuisine_name_prompt)\n",
        "\n",
        "\n",
        "items_template = \"Suggest some menu item for {restaurant_name}. Return it as comma seprate list.\"\n",
        "item_prompt = PromptTemplate(template=items_template, input_variables= [\"restaurant_name\"])\n",
        "item_chain = LLMChain(llm=llm, prompt=item_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v2fF9fQklPgc",
        "outputId": "0d45ab65-e0f3-404b-f0d0-ec76fbd7d86c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['shanghai','shanghai','shang\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "chain = SimpleSequentialChain(chains= [cuisine_name_chain, item_chain])\n",
        "response = chain.run([\"chinese\"])\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIHfktmHrQvo"
      },
      "source": [
        "SequentialChain *chain*  \n",
        "\n",
        "SequentialChain =======\n",
        "\n",
        " Input(India restaurent) -->(restaurent name) --> Output(Maharaja) ---> (GIVE ME food menu items) ---> lassi, tandoori, tea, samosa\n",
        "\n",
        " It can give more than one output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBJvWYmrZhR",
        "outputId": "623bc9a3-4728-4157-b509-a4806075f66a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cuisine': 'japani', 'menu_items': ',,,, '}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "\n",
        "cuisine_name_template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        "cuisine_name_prompt = PromptTemplate(template=cuisine_name_template, input_variables= [\"cuisine\"])\n",
        "cuisine_name_chain = LLMChain(llm=llm, prompt=cuisine_name_prompt, output_key=\"restaurent_name\")\n",
        "\n",
        "items_template = \"Suggest some menu item for {restaurent_name}. Return it as comma seprate list.\"\n",
        "item_prompt = PromptTemplate(template=items_template, input_variables= [\"restaurant_name\"])\n",
        "item_chain = LLMChain(llm=llm, prompt=item_prompt, output_key=\"menu_items\")\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(chains= [cuisine_name_chain, item_chain], input_variables=['cuisine'],\n",
        "                        output_variables=['menu_items'])\n",
        "response = chain({'cuisine':'japani'})\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7y2sOzYPYr2",
        "outputId": "b77dc8ab-ad0f-4a3a-fb01-47b20aa1e7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Tragedy at sunset on the beach',\n",
              " 'era': 'Victorian England',\n",
              " 'synopsis': 'A young woman, suffering from tuberculosis, is raped by her',\n",
              " 'review': 'a play that is too preoccupied with its own enigmatic nature to be fully appreciated'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "synopsis_template = \"\"\"You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\n",
        "\n",
        "Title: {title}\n",
        "Era: {era}\n",
        "Playwright: This is a synopsis for the above play:\"\"\"\n",
        "synopsis_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"title\", \"era\"], template=synopsis_template\n",
        ")\n",
        "synopsis_chain = LLMChain(\n",
        "    llm=llm, prompt=synopsis_prompt_template, output_key=\"synopsis\"\n",
        ")\n",
        "\n",
        "\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "\n",
        "Play Synopsis:\n",
        "{synopsis}\n",
        "Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"review\")\n",
        "\n",
        "# This is the overall chain where we run these two chains in sequence.\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        "    input_variables=[\"era\", \"title\"],\n",
        "    # Here we return multiple variables\n",
        "    output_variables=[\"synopsis\", \"review\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "\n",
        "overall_chain({\"title\": \"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Agents*"
      ],
      "metadata": {
        "id": "ryE4EL1Rp4YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm = llm)\n",
        "agent = initialize_agent(tools = tools, llm = llm, agent= AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True)\n",
        "test = agent.run('when was elon musk born ? what is his age in 2023 ?' )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XggUkbHBklkJ",
        "outputId": "7ba745b7-3488-47ab-cc1c-4a82236c4116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mwikipaedia elon musk age in 2023 is 66 years\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mwikipaedia elon musk age in 2023 is 66 years\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:I now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:I now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:I now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:I now know the final\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:Thought:I\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:Thought:Th\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:Thought:Th\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:Thought:Th\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:Thought:Th\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:Thought:Thought:Thought:Thought:Th\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# SERPapi**"
      ],
      "metadata": {
        "id": "Q3MgtXkH4A0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "tools = load_tools([\"serpapi\"], llm = llm)\n",
        "agent = initialize_agent(tools = tools, llm = llm, agent= AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True)\n",
        "test = agent.run('What was the GDP of US in 2022 ?' )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udHthMLX4FSK",
        "outputId": "eeb5b371-95dc-4408-847f-3b529c6d4290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mSearch for \"US GDP 2022\" Action: Input \"US GDP 2022\" Observ\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer'\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer'\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer' Observation: Invalid Format: Missing 'A\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MEMORY***"
      ],
      "metadata": {
        "id": "S-YkwS8n7aRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ConversationBufferMemory***"
      ],
      "metadata": {
        "id": "seCiOv5oYNEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "template = \"When was {company} founded? \"\n",
        "prompt_template_company = PromptTemplate(input_variables =[\"company\"], template = template)\n",
        "prompt_template_company.format(company=\"apple\")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm = llm, prompt = prompt_template_company, memory=memory) #here is magic\n",
        "chain.run(\"ibm\")\n",
        "\n",
        "chain.memory\n",
        "chain.run(\"apple\")\n",
        "chain.memory\n",
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYOXCr5T7dYF",
        "outputId": "13ae0922-02a5-4f7e-c7ac-859622ef8e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: ibm\n",
            "AI: 1890\n",
            "Human: apple\n",
            "AI: September 1976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ConversationChain***"
      ],
      "metadata": {
        "id": "l4KkoMoEYSkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm =llm)\n",
        "print(convo.prompt.template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOC_F4LPYV5R",
        "outputId": "6317fc64-4806-48e3-fb42-aa394821276d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('who won first football world cup')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OIX7m8hvbSmC",
        "outputId": "392172f0-6931-4935-9884-dc3fb34bbb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Brazil'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('what is 5 plus 5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lr53WnXWboq6",
        "outputId": "fa13afa0-59e1-4b00-bd0d-e755545f689f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5+5 is a family of quilt patterns with the same back and front, but different quilting designs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(convo.run('who was the captian of winning team'))\n",
        "\n",
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dot-3bFNbYK_",
        "outputId": "9c9a9316-e522-47f4-d32d-b1a77fbac5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Luiz Felipe Scolari\n",
            "Human: who won first football world cup\n",
            "AI: Brazil\n",
            "Human: who was the captian of winning team\n",
            "AI: Luiz Felipe Scolari\n",
            "Human: who was the captian of winning team\n",
            "AI: Luiz Felipe Scolari\n",
            "Human: what is 5+5\n",
            "AI: 5+5 is a family of quilt patterns with the same back and front, but different quilting designs.\n",
            "Human: what is 5 plus 5\n",
            "AI: 5+5 is a family of quilt patterns with the same back and front, but different quilting designs.\n",
            "Human: who won first football world cup\n",
            "AI: Brazil\n",
            "Human: what is 5 plus 5\n",
            "AI: 5+5 is a family of quilt patterns with the same back and front, but different quilting designs.\n",
            "Human: who was the captian of winning team\n",
            "AI: Luiz Felipe Scolari\n",
            "Human: who was the captian of winning team\n",
            "AI: Luiz Felipe Scolari\n",
            "Human: who was the captian of winning team\n",
            "AI: Luiz Felipe Scolari\n",
            "Human: who was the captian of winning team\n",
            "AI: Luiz Felipe Scolari\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConersationBufferWindowMemory"
      ],
      "metadata": {
        "id": "d8VckGPmcj9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2)\n",
        "\n",
        "convo = ConversationChain(llm =llm, memory=memory)\n",
        "\n",
        "convo.run('who won first football world cup')\n",
        "convo.run('what is 5 plus 5')\n",
        "convo.run('who was the captian of winning team')\n",
        "print(convo.memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCjDIwNDcnyA",
        "outputId": "1e83afdf-c06a-42e3-9a07-47cd3cb66d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='who won first football world cup'), AIMessage(content='Brazil'), HumanMessage(content='what is 5 plus 5'), AIMessage(content='10'), HumanMessage(content='who was the captian of winning team'), AIMessage(content='Paulista')]) k=2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTFLe6yDSzTlsFdtenar6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}