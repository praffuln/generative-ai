{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQAdMlZ3/EYmj19BsFBWyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/generative-ai/blob/master/rag/rag-1/rag_routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E26By-JRgmO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8461135d-745b-4904-dc71-2128b2838ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "print('hello world')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enviornment**"
      ],
      "metadata": {
        "id": "D8Wf0z9wRDM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube google-generativeai langchain_google_genai langchain_community"
      ],
      "metadata": {
        "id": "AekDM0f7Q_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Setup Keys For GOOGLE_API_KEY AND LANGSMITH"
      ],
      "metadata": {
        "id": "tzaWEi7vRMJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning value to variable\n",
        "GOOGLE_API_KEY=''\n",
        "LANGCHAIN_API_KEY = ''"
      ],
      "metadata": {
        "id": "dvFOprifTZNj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports os, google Gemini EMbedding and Chat Classes with FAISS vectorstores"
      ],
      "metadata": {
        "id": "uoqtF2Z2N8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "ALaWrOJvc3Qe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup langsmith for tracing"
      ],
      "metadata": {
        "id": "6Y_EPIyLOH0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "NSYmU7-pb0wM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Logical and Semantic routing]\n",
        "\n",
        "Use function-calling for classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "K_wDozkdzHPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3) # Try using gemini-pro-vision\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source by selecting one of the following options:\n",
        "\n",
        "python_docs\n",
        "js_docs\n",
        "golang_docs\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm\n",
        "print(router)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJf7rL1CzJWR",
        "outputId": "bdba6827-e927-4fb4-d1f1-23d3a5243130"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first=ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at routing a user question to the appropriate data source.\\n\\nBased on the programming language the question is referring to, route it to the relevant data source by selecting one of the following options:\\n\\npython_docs\\njs_docs\\ngolang_docs\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]) middle=[RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.3, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e57e83caef0>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7e57e84460e0>, default_metadata=()), kwargs={'tools': [{'function_declarations': [{'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'type_': 6, 'properties': {'datasource': {'type_': 1, 'description': 'Given a user question choose which datasource would be most relevant for answering their question', 'format_': '', 'nullable': False, 'enum': [], 'properties': {}, 'required': []}}, 'required': ['datasource'], 'format_': '', 'description': '', 'nullable': False, 'enum': []}}]}], 'tool_config': None})] last=PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFuaSefD0Y_i",
        "outputId": "f58334f2-74f7-42e3-f103-08e72a20afde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='python_docs'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvwfllXe3Dm8",
        "outputId": "9d9855cd-0c96-4d2a-baeb-f40a46e23ce7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RouteQuery(datasource='python_docs')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.datasource\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_asmcBJA3KGR",
        "outputId": "0aab57c7-4486-4dcb-a89c-8262c9c05a54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'python_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"golang_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)\n"
      ],
      "metadata": {
        "id": "pnH9IHgL3VRI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\": question})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pRVsZO0i3YV4",
        "outputId": "c11ae30f-dc56-407b-f5e1-a0b93aa3c55a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chain for python_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Example"
      ],
      "metadata": {
        "id": "YYRZGeP0v1hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"product_index\", \"customer_index\", \"order_index\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3) # Try using gemini-pro-vision\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the question please suggest which datasource should we use for retrival:\n",
        "\n",
        "product_index\n",
        "customer_index\n",
        "order_index\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm\n",
        "print(router)\n",
        "\n",
        "\n",
        "question = \"\"\"\n",
        "I am website visitor, I need to find out product information for your website. Please help me.\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})\n",
        "print('result when question is related to product')\n",
        "print(result)\n",
        "\n",
        "\n",
        "question = \"\"\"\n",
        "I am website visitor, I have ordered some products on website, to find out more relative information about them. Please help me.\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})\n",
        "print('result when question is related to order')\n",
        "print(result)\n",
        "\n",
        "\n",
        "def choose_route(result):\n",
        "    if \"product_index\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for product_index\"\n",
        "    elif \"order_index\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for order_index\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"customer_index\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)\n",
        "\n",
        "print('\\n\\nfinal answer based on question with its retrival')\n",
        "print(full_chain.invoke({\"question\": question}))\n"
      ],
      "metadata": {
        "id": "rw3yhdLvv2qG",
        "outputId": "9a6bccd5-c40a-438a-957c-788fe5d42051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first=ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at routing a user question to the appropriate data source.\\n\\nBased on the question please suggest which datasource should we use for retrival:\\n\\nproduct_index\\ncustomer_index\\norder_index\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]) middle=[RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.3, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7e57e4699e10>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7e57e469b7c0>, default_metadata=()), kwargs={'tools': [{'function_declarations': [{'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'type_': 6, 'properties': {'datasource': {'type_': 1, 'description': 'Given a user question choose which datasource would be most relevant for answering their question', 'format_': '', 'nullable': False, 'enum': [], 'properties': {}, 'required': []}}, 'required': ['datasource'], 'format_': '', 'description': '', 'nullable': False, 'enum': []}}]}], 'tool_config': None})] last=PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])\n",
            "result when question is related to product\n",
            "datasource='product_index'\n",
            "result when question is related to order\n",
            "datasource='order_index'\n",
            "\n",
            "\n",
            "final answer based on question with its retrival\n",
            "chain for order_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic routing\n"
      ],
      "metadata": {
        "id": "Dod7GvWN6EnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Two prompts\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "# Embed prompts\n",
        "# Create embeddings using a Google Generative AI model\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Route question to prompt\n",
        "def prompt_router(input):\n",
        "    # Embed question\n",
        "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "    # Compute similarity\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    print(similarity)\n",
        "    print(similarity.argmax())\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    print(most_similar)\n",
        "    # Chosen prompt\n",
        "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"What's a black hole\"))\n",
        "\n",
        "print(chain.invoke(\"What's multiplication of 4 and 2\"))\n"
      ],
      "metadata": {
        "id": "HbwHAJrd6Hin",
        "outputId": "9aa52d4b-1c52-4ce1-b73e-772c3c02eeef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.59911763 0.58184552]\n",
            "0\n",
            "You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.\n",
            "\n",
            "Here is a question:\n",
            "{query}\n",
            "Using PHYSICS\n",
            "A black hole is a region of spacetime where gravity is so strong that nothing, not even light, can escape. It's formed when a massive star collapses at the end of its life. \n",
            "\n",
            "Imagine a giant ball of super-dense matter, so dense that its gravity warps spacetime around it like a bowling ball on a trampoline. This warping is so extreme that it creates a point of no return, called the event horizon.  Anything that crosses the event horizon is trapped forever, even light. \n",
            "\n",
            "While we can't see black holes directly because they don't emit light, we can detect their presence by observing their gravitational influence on nearby objects. \n",
            "\n",
            "[0.57641401 0.62754677]\n",
            "1\n",
            "You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\n",
            "\n",
            "Here is a question:\n",
            "{query}\n",
            "Using MATH\n",
            "Multiplication of 4 and 2 is simply **8**. \n",
            "\n",
            "Here's how we break it down:\n",
            "\n",
            "* **Component 1:**  We understand that multiplication is repeated addition.\n",
            "* **Component 2:**  We know that 4 multiplied by 2 means adding 4 to itself 2 times.\n",
            "* **Component 3:**  4 + 4 = 8\n",
            "\n",
            "Therefore, the answer is 8. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Construction"
      ],
      "metadata": {
        "id": "suM8zCJADv0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query structuring for metadata filters"
      ],
      "metadata": {
        "id": "wTzXgWtwDyEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many vectorstores contain metadata fields.\n",
        "\n",
        "This makes it possible to filter for specific chunks based on metadata.\n",
        "\n",
        "Let's look at some example metadata we might see in a database of YouTube transcripts.\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/query_analysis/techniques/structuring\n",
        "\n"
      ],
      "metadata": {
        "id": "0K5Ak4-AWXQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "docs = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
        ").load()\n",
        "\n",
        "docs[0].metadata\n"
      ],
      "metadata": {
        "id": "igUwgA54D3he",
        "outputId": "af8b5d15-9a91-4b45-9446-5c1214cf3b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'pbAd8O1Lvm4',\n",
              " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 22149,\n",
              " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
              " 'publish_date': '2024-02-07 00:00:00',\n",
              " 'length': 1058,\n",
              " 'author': 'LangChain'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s assume we’ve built an index that:\n",
        "\n",
        "Allows us to perform unstructured search over the contents and title of each document\n",
        "And to use range filtering on view count, publication date, and length.\n",
        "We want to convert natural langugae into structured search queries.\n",
        "\n",
        "We can define a schema for structured search queries.\n",
        "\n"
      ],
      "metadata": {
        "id": "QofJXA4NWvbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class TutorialSearch(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    content_search: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    title_search: str = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Alternate version of the content search query to apply to video titles. \"\n",
        "            \"Should be succinct and only include key words that could be in a video \"\n",
        "            \"title.\"\n",
        "        ),\n",
        "    )\n",
        "    min_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    earliest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    latest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    min_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "\n",
        "    def pretty_print(self) -> None:\n",
        "        for field in self.__fields__:\n",
        "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
        "                self.__fields__[field], \"default\", None\n",
        "            ):\n",
        "                print(f\"{field}: {getattr(self, field)}\")\n"
      ],
      "metadata": {
        "id": "SKl350_eWxJ0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a database query optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "structured_llm = llm.with_structured_output(TutorialSearch)\n",
        "query_analyzer = prompt | structured_llm\n"
      ],
      "metadata": {
        "id": "LhyaBrd6XWRH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()\n"
      ],
      "metadata": {
        "id": "KgYXuVpmXhDR",
        "outputId": "df47687d-29dc-4802-83a6-437a30ec02be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_search: rag from scratch\n",
            "title_search: rag from scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\"}\n",
        ").pretty_print()\n"
      ],
      "metadata": {
        "id": "O4ViIDueXukk",
        "outputId": "779bbb0a-29da-414d-9bbf-6a99d24f18b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_search: chat langchain\n",
            "title_search: chat langchain\n",
            "latest_publish_date: 2024-01-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
        "    }\n",
        ").pretty_print()\n"
      ],
      "metadata": {
        "id": "msgX6edeYYXC",
        "outputId": "0731c5f8-9d1a-4165-c3ff-94ad4364521b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_search: multi-modal models agent\n",
            "title_search: multi-modal agent\n",
            "max_length_sec: 300\n"
          ]
        }
      ]
    }
  ]
}