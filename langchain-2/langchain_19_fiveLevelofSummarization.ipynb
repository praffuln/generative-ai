{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNawqaKCKJen41vkrw6TdQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/langchain/blob/master/langchain-2/langchain_19_fiveLevelofSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxcwN0lQ90Lp"
      },
      "outputs": [],
      "source": [
        "### I am Incomplete\n",
        "\n",
        "!pip install langchain==0.0.350\n",
        "!pip install openAI\n",
        "!pip install wikipedia\n",
        "!pip install huggingface_hub\n",
        "!pip install InstructorEmbedding\n",
        "!pip install google-search-results\n",
        "!pip install unstructured\n",
        "!pip install libmagic\n",
        "!pip install python-magic\n",
        "!pip install python-magic-bin\n",
        "#Install faiss Packages\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install wolframalpha\n",
        "!pip install pypdf\n",
        "!pip install youtube-transcript-api\n",
        "!pip install pytube\n",
        "!pip install python-dotenv\n",
        "!pip install pinecone-client\n",
        "!pip install kor\n",
        "!pip install markdownify\n",
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API"
      ],
      "metadata": {
        "id": "c67GeoptLNsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "api_key = ''    # get this free api key from https://makersuite.google.com/\n"
      ],
      "metadata": {
        "id": "iTI8L6-19ygG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Palm LLM\n"
      ],
      "metadata": {
        "id": "usqtWViMgvJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load up LLM\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain_core._api import deprecation\n",
        "\n",
        "!pip install --upgrade vertexai google-cloud-aiplatform\n",
        "!pip install google-generativeai\n",
        "\n",
        "llm = GooglePalm(google_api_key=api_key, temperature=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "B1sdUzs3gxpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqm63X4oPWhV",
        "outputId": "a8139d3e-4d54-47d1-cc9d-22337796585e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.350\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: kor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test LLM setup\n",
        "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
        "\n",
        "print(poem)\n"
      ],
      "metadata": {
        "id": "mXO6dK5425aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Levels Of Summarization: Novice to Expert\n",
        "Summarization is a fundamental building block of many LLM tasks. You'll frequently run into use cases where you would like to distill a large body of text into a succinct set of points.\n",
        "\n",
        "Depending on the length of the text you'd like to summarize, you have different summarization methods to choose from.\n",
        "\n",
        "We're going to run through 5 methods for summarization that start with Novice and end up expert. These aren't the only options, feel free to make up your own. If you find another one you like please share it with the community.\n",
        "\n",
        "\n",
        "**5 Levels Of Summarization:**\n",
        "\n",
        "Summarize a couple sentences - Basic Prompt\n",
        "\n",
        "Summarize a couple paragraphs - Prompt Templates\n",
        "\n",
        "Summarize a couple pages - Map Reduce\n",
        "\n",
        "Summarize an entire book - Best Representation Vectors\n",
        "\n",
        "Summarize an unknown amount of text - Agents\n"
      ],
      "metadata": {
        "id": "Zp6FeeOj-UBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Level 1: Basic Prompt - Summarize a couple sentences\n",
        "If you just have a few sentences you want to one-off summarize you can use a simple prompt and copy and paste your text.\n",
        "\n",
        "This method isn't scalable and only practical for a few use cases...the perfect level #1!\n",
        "\n",
        "The important part is to provide instructions for the LLM to know what to do. In thise case I'm telling the model I want a summary of the text below\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LTJndSkY_IJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Please provide a summary of the following text\n",
        "\n",
        "TEXT:\n",
        "Philosophy (from Greek: œÜŒπŒªŒøœÉŒøœÜŒØŒ±, philosophia, 'love of wisdom') \\\n",
        "is the systematized study of general and fundamental questions, \\\n",
        "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
        "Some sources claim the term was coined by Pythagoras (c.‚Äâ570 ‚Äì c.‚Äâ495 BCE), \\\n",
        "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
        "critical discussion, rational argument, and systematic presentation.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "-scTeJO4_M7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = llm.get_num_tokens(prompt)\n",
        "print (f\"Our prompt has {num_tokens} tokens\")\n"
      ],
      "metadata": {
        "id": "-8hQwlKv_P9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(prompt)\n",
        "print (output)\n"
      ],
      "metadata": {
        "id": "2PqF6F8O_W6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Woof üê∂, that summary is still hard to understand. Let me add to my instructions so that the output is easier to understand. I'll tell it to explain it to me like a 5 year old.\n",
        "\n"
      ],
      "metadata": {
        "id": "37QqPat9_kTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Please provide a summary of the following text.\n",
        "Please provide your output in a manner that a 5 year old would understand\n",
        "\n",
        "TEXT:\n",
        "Philosophy (from Greek: œÜŒπŒªŒøœÉŒøœÜŒØŒ±, philosophia, 'love of wisdom') \\\n",
        "is the systematized study of general and fundamental questions, \\\n",
        "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
        "Some sources claim the term was coined by Pythagoras (c.‚Äâ570 ‚Äì c.‚Äâ495 BCE), \\\n",
        "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
        "critical discussion, rational argument, and systematic presentation.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "bIbP4mDo_lZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = llm.get_num_tokens(prompt)\n",
        "print (f\"Our prompt has {num_tokens} tokens\")\n"
      ],
      "metadata": {
        "id": "k1vc_0u3_oc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(prompt)\n",
        "print (output)\n"
      ],
      "metadata": {
        "id": "ZEi4h8HB_rbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Level 2: Prompt Templates - Summarize a couple paragraphs\n",
        "\n",
        "Prompt templates are a great way to dynamically place text within your prompts. They are like python f-strings but specialized for working with language models.\n",
        "\n",
        "We're going to look at 2 short Paul Graham essays\n",
        "\n"
      ],
      "metadata": {
        "id": "W1B-pV7-A0bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "import os\n"
      ],
      "metadata": {
        "id": "ey-lC9GlA-U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paul_graham_essays = ['/content/sample_data/getideas.txt', '/content/sample_data/noob.txt']\n",
        "\n",
        "essays = []\n",
        "\n",
        "for file_name in paul_graham_essays:\n",
        "    with open(file_name, 'r') as file:\n",
        "        essays.append(file.read())\n"
      ],
      "metadata": {
        "id": "hu3xTq26A_hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, essay in enumerate(essays):\n",
        "    print (f\"Essay #{i+1}: {essay[:300]}\\n\")\n"
      ],
      "metadata": {
        "id": "9CCRCm_ABDj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's create a prompt template which will hold our instructions and a placeholder for the essay. In this example I only want a 1 sentence summary to come back\n",
        "\n"
      ],
      "metadata": {
        "id": "kBN9UwAABK8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Please write a one sentence summary of the following text:\n",
        "\n",
        "{essay}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"essay\"],\n",
        "    template=template\n",
        ")\n"
      ],
      "metadata": {
        "id": "9vXPm_IiBMDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then let's loop through the 2 essays and pass them to our LLM. I'm applying .strip() on the summaries to remove the white space on the front and back of the output\n",
        "\n"
      ],
      "metadata": {
        "id": "KVYlntNNBORP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for essay in essays:\n",
        "    print(essay)\n",
        "    summary_prompt = prompt.format(essay=essay)\n",
        "\n",
        "    num_tokens = llm.get_num_tokens(summary_prompt)\n",
        "    print (f\"This prompt + essay has {num_tokens} tokens\")\n",
        "\n",
        "    summary = llm(summary_prompt)\n",
        "\n",
        "    print (f\"Summary: {summary.strip()}\")\n",
        "    print (\"\\n\")\n"
      ],
      "metadata": {
        "id": "5veCnPmYBQfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Level 3: Map Reduce - Summarize a couple pages multiple pages\n",
        "\n",
        "If you have multiple pages you'd like to summarize, you'll likely run into a token limit. Token limits won't always be a problem, but it is good to know how to handle them if you run into the issue.\n",
        "\n",
        "The chain type \"Map Reduce\" is a method that helps with this. You first generate a summary of smaller chunks (that fit within the token limit) and then you get a summary of the summaries.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eJ2zYnFwBS8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "0m9l3tlqBd4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paul_graham_essay = '/content/sample_data/startupideas.txt'\n",
        "\n",
        "with open(paul_graham_essay, 'r') as file:\n",
        "    essay = file.read()\n"
      ],
      "metadata": {
        "id": "roTv5ml3ltOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(essay)\n"
      ],
      "metadata": {
        "id": "kSrGzii4l9q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's too many, let's split our text up into chunks so they fit into the prompt limit. I'm going a chunk size of 10,000 characters.\n",
        "\n",
        "\n",
        "\n",
        ">   You can think of tokens as pieces of words used for natural language processing. For English text, 1 token is approximately 4 characters or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.\n",
        "\n",
        "\n",
        "\n",
        "This means the number of tokens we should expect is 10,000 / 4 = ~2,500 token chunks. But this will vary, each body of text/code will be different\n",
        "\n"
      ],
      "metadata": {
        "id": "pKeEdrzbmC6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=500, chunk_overlap=500)\n",
        "\n",
        "docs = text_splitter.create_documents([essay])\n"
      ],
      "metadata": {
        "id": "UtrJ6BT0nPS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_docs = len(docs)\n",
        "\n",
        "num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
        "\n",
        "print (f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n"
      ],
      "metadata": {
        "id": "ig3lMknGnW02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce',\n",
        "#                                      verbose=True # Set verbose=True if you want to see the prompts being used\n",
        "                                    )\n"
      ],
      "metadata": {
        "id": "njZqlQdCnZSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = summary_chain.run(docs)\n",
        "output"
      ],
      "metadata": {
        "id": "yTJELT5MneZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This summary is a great start, but I'm more of a bullet point person. I want to get my final output in bullet point form.\n",
        "\n",
        "In order to do this I'm going to use custom promopts (like we did above) to instruct the model on what I want.\n",
        "\n",
        "The map_prompt is going to stay the same (just showing it for clarity), but I'll edit the combine_prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "IglJQLBotroI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_prompt = \"\"\"\n",
        "Write a concise summary of the following:\n",
        "\"{text}\"\n",
        "CONCISE SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n"
      ],
      "metadata": {
        "id": "iDz7r2X-tvzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_prompt = \"\"\"\n",
        "Write a concise summary of the following text delimited by triple backquotes.\n",
        "Return your response in bullet points which covers the key points of the text.\n",
        "```{text}```\n",
        "BULLET POINT SUMMARY:\n",
        "\"\"\"\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n"
      ],
      "metadata": {
        "id": "v0FyrnStt3R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_chain = load_summarize_chain(llm=llm,\n",
        "                                     chain_type='map_reduce',\n",
        "                                     map_prompt=map_prompt_template,\n",
        "                                     combine_prompt=combine_prompt_template,\n",
        "                                     verbose=True\n",
        "                                    )\n"
      ],
      "metadata": {
        "id": "LVw1UyW7u75k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = summary_chain.run(docs)\n"
      ],
      "metadata": {
        "id": "cKNi84TGvQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (output)\n"
      ],
      "metadata": {
        "id": "UxyTsFdwvbmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Level 4: Best Representation Vectors - Summarize an entire book\n",
        "In the above method we pass the entire document (all 9.5K tokens of it) to the LLM. But what if you have more tokens than that?\n",
        "\n",
        "What if you had a book you wanted to summarize? Let's load one up, we're going to load Into Thin Air about the 1996 Everest Disaster\n",
        "\n"
      ],
      "metadata": {
        "id": "EhLjJDuS25xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the book\n",
        "loader = PyPDFLoader(\"/content/sample_data/IntoThinAirBook.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "# Cut out the open and closing parts\n",
        "pages = pages[26:277]\n",
        "\n",
        "# Combine the pages, and replace the tabs with spaces\n",
        "text = \"\"\n",
        "\n",
        "for page in pages:\n",
        "    text += page.page_content\n",
        "\n",
        "text = text.replace('\\t', ' ')\n"
      ],
      "metadata": {
        "id": "KuFKZXM928j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[0:50000]\n",
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"This book has {num_tokens} tokens in it\")\n"
      ],
      "metadata": {
        "id": "aHusEhFo6GHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, that's over 100K tokens, even GPT 32K wouldn't be able to handle that in one go. At 0.03 per 1K prompt tokens, this would cost us $4.17 just for the prompt alone.\n",
        "\n",
        "So how do we do this without going through all the tokens? Pick random chunks? Pick equally spaced chunks?\n",
        "\n",
        "I kicked off a twitter thread with a proposed solution to see if I was off base. I'm calling it the Best Representation Vectors method (not sure if a name already exists for it).\n",
        "\n",
        "Goal: Chunk your book then get embeddings of the chunks. Pick a subset of chunks which represent a wholistic but diverse view of the book. Or another way, is there a way to pick the top 10 passages that describe the book the best?\n",
        "\n",
        "Once we have our chunks that represent the book then we can summarize those chunks and hopefully get a pretty good summary.\n",
        "\n",
        "Keep in mind there are tools that would likely do this for you, and with token limits increasing this won't be a problem for long. But if you want to do it from scratch this might help.\n",
        "\n",
        "This is most definitely not the optimal answer, but it's my take on it for now! If the clustering experts wanna help improve it that would be awesome.\n",
        "\n",
        "The BRV Steps:\n",
        "\n",
        "Load your book into a single text file\n",
        "Split your text into large-ish chunks\n",
        "Embed your chunks to get vectors\n",
        "Cluster the vectors to see which are similar to each other and likely talk about the same parts of the book\n",
        "Pick embeddings that represent the cluster the most (method: closest to each cluster centroid)\n",
        "Summarize the documents that these embeddings represent\n",
        "Another way to phrase this process, \"Which ~10 documents from this book represent most of the meaning? I want to build a summary off those.\"\n",
        "\n",
        "Note: There will be a bit of information loss, but show me a summary of a whole book that doesn't have information loss ;)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ohv9Or8s6R2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n"
      ],
      "metadata": {
        "id": "n5x1XU7G6TC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=10000, chunk_overlap=3000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])\n"
      ],
      "metadata": {
        "id": "Bwh9A8QR9w08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")\n"
      ],
      "metadata": {
        "id": "lc1USquQ9zat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google AI chat models\n"
      ],
      "metadata": {
        "id": "DS1V9IWLAeTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai pillow"
      ],
      "metadata": {
        "id": "uDfYjB3WAkyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = ''\n",
        "\n"
      ],
      "metadata": {
        "id": "cEVJZkoPAtdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n"
      ],
      "metadata": {
        "id": "dO7-6-dHA7QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(\"Write a ballad about LangChain\")\n",
        "print(result.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "oxMvrCL-BA3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(\"Write a limerick about LLMs.\"):\n",
        "    print(chunk.content)\n",
        "    print(\"---\")\n",
        "# Note that each chunk may contain more than one \"token\"\n",
        "\n"
      ],
      "metadata": {
        "id": "FzCM1qpWCFXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = llm.batch(\n",
        "    [\n",
        "        \"What's 2+2?\",\n",
        "        \"What's 3+5?\",\n",
        "    ]\n",
        ")\n",
        "for res in results:\n",
        "    print(res.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "tpqHqX_CCNMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from IPython.display import Image\n",
        "\n",
        "image_url = \"https://picsum.photos/seed/picsum/300/300\"\n",
        "content = requests.get(image_url).content\n",
        "Image(content)\n",
        "\n"
      ],
      "metadata": {
        "id": "QXXT7fhPChLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
        "# example\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"What's in this image?\",\n",
        "        },  # You can optionally provide text parts\n",
        "        {\"type\": \"image_url\", \"image_url\": image_url},\n",
        "    ]\n",
        ")\n",
        "llm.invoke([message])\n",
        "\n"
      ],
      "metadata": {
        "id": "2o7vx4kgCju3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}