{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzGP2g8iqT6l2jB2XA43NB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/generative-ai/blob/master/rag/rag-1/rag_indexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Ajnmz8ZfEJ",
        "outputId": "e3b98e10-d607-422f-af73-62c073f49273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world!!\n"
          ]
        }
      ],
      "source": [
        "print('hello world!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing"
      ],
      "metadata": {
        "id": "lJre_8n7aV1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-representation Indexing"
      ],
      "metadata": {
        "id": "xR6JWQW2gLvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enviornment**"
      ],
      "metadata": {
        "id": "D8Wf0z9wRDM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain-chroma langchain youtube-transcript-api pytube google-generativeai langchain_google_genai langchain_community"
      ],
      "metadata": {
        "id": "AekDM0f7Q_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Setup Keys For GOOGLE_API_KEY AND LANGSMITH"
      ],
      "metadata": {
        "id": "tzaWEi7vRMJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning value to variable\n",
        "GOOGLE_API_KEY=''\n",
        "LANGCHAIN_API_KEY = ''"
      ],
      "metadata": {
        "id": "dvFOprifTZNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports os, google Gemini EMbedding and Chat Classes with FAISS vectorstores"
      ],
      "metadata": {
        "id": "uoqtF2Z2N8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "ALaWrOJvc3Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup langsmith for tracing"
      ],
      "metadata": {
        "id": "6Y_EPIyLOH0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "NSYmU7-pb0wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize LLM And Embedding"
      ],
      "metadata": {
        "id": "qvscJ6UAjmHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM with function call\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "\n",
        "# Create embeddings using a Google Generative AI model\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mGgzDqI3jq0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch the documents"
      ],
      "metadata": {
        "id": "3FqwXYGVj5FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
        "docs.extend(loader.load())\n",
        "\n",
        "print(len(docs))\n",
        "print('\\nprinting metadata...\\n')\n",
        "for doc in docs:\n",
        "  print(doc.metadata)\n",
        "\n"
      ],
      "metadata": {
        "id": "aeNPRO6Hj7Mt",
        "outputId": "04d42939-9619-4e1e-8e3f-c4eefad01b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "\n",
            "printing metadata...\n",
            "\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}\n",
            "{'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'title': \"Thinking about High-Quality Human Data | Lil'Log\", 'description': '[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper “Vox populi”) and nice feedback. \\uf8ffüôè ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.', 'language': 'en'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
        "\n",
        "len(summaries)\n",
        "for summary in summaries:\n",
        "  print(summary)\n",
        "  print('\\n\\n\\n\\n\\n\\n\\n')"
      ],
      "metadata": {
        "id": "mEOwdVKGl379"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Store summary in vectorstore and original Docs in byteStore"
      ],
      "metadata": {
        "id": "DDihvnMooLl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import InMemoryByteStore\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\",\n",
        "                     embedding_function=embeddings)\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "\n",
        "# Docs linked to summaries\n",
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]\n",
        "\n",
        "\n",
        "summary_docs"
      ],
      "metadata": {
        "id": "iXQnXkN-oOi2",
        "outputId": "f9c8bf59-03e3-42aa-f020-7149b424f44d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'doc_id': '9d570c6f-5a6f-4478-b18f-c8131a41d52b'}, page_content='This document explores the concept of LLM-powered autonomous agents, which are systems using large language models (LLMs) as their core controllers. It breaks down the key components of such agents: planning, memory, and tool use.\\n\\n**Planning** involves breaking down complex tasks into smaller subgoals and reflecting on past actions to improve future performance. Techniques like Chain of Thought (CoT), Tree of Thoughts, and ReAct are discussed for task decomposition and self-reflection.\\n\\n**Memory** encompasses short-term memory (in-context learning) and long-term memory (external vector stores). The document delves into the concept of Maximum Inner Product Search (MIPS) and various algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN for fast retrieval from vector stores.\\n\\n**Tool Use** equips LLMs with the ability to interact with external APIs and tools. Examples like MRKL, TALM, Toolformer, ChatGPT Plugins, OpenAI API function calling, and HuggingGPT are presented, showcasing how LLMs can leverage external resources to enhance their capabilities.\\n\\nThe document also discusses case studies like ChemCrow (a scientific discovery agent) and Generative Agents (a simulation of virtual characters interacting in a sandbox environment). It highlights proof-of-concept examples like AutoGPT and GPT-Engineer, which demonstrate the potential of LLMs for automating tasks and generating code.\\n\\nFinally, the document acknowledges challenges associated with LLM-powered agents, including finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes by emphasizing the ongoing research and development in this field, aiming to overcome these limitations and unlock the full potential of autonomous agents powered by LLMs.\\n'),\n",
              " Document(metadata={'doc_id': '9664a615-8ee1-4e1d-9f8b-12416ae95ff6'}, page_content='This document explores the importance of high-quality human data in deep learning model training, focusing on the challenges and opportunities associated with collecting, evaluating, and utilizing human annotations. \\n\\n**Key Takeaways:**\\n\\n* **Human data quality is crucial:**  The document emphasizes that high-quality human data is essential for training effective deep learning models, particularly in tasks involving classification and reinforcement learning from human feedback (RLHF).\\n* **The \"wisdom of the crowd\" has limitations:** While crowdsourcing can be cost-effective, it\\'s important to be aware of potential issues like spammers and the need for careful weighting schemes to ensure data quality.\\n* **Rater agreement is a complex issue:**  The document discusses various methods for measuring rater agreement, including majority voting, Cohen\\'s Kappa, and probabilistic graph modeling. It highlights the importance of addressing rater disagreement, particularly in subjective tasks where multiple interpretations may be valid.\\n* **Two contrasting paradigms for data annotation:**  The document presents two paradigms for data annotation: descriptive (embracing subjectivity) and prescriptive (encouraging consistency). It argues that the choice of paradigm depends on the specific task and desired outcome.\\n* **Data quality can be assessed during model training:**  The document explores methods for identifying mislabeled data during model training, including influence functions, prediction changes during training, and noisy cross-validation.\\n\\n**Key Methods and Techniques:**\\n\\n* **Influence functions:**  A technique for measuring the impact of individual data points on model parameters and loss function.\\n* **Data Maps:**  A method for tracking model confidence and variability during training to identify potentially mislabeled data.\\n* **AUM (Area under the Margin):**  A method for identifying mislabeled data by analyzing the margin between the assigned logit and the next largest logit during training.\\n* **NCV (Noisy Cross-Validation):**  A method for identifying clean data samples by comparing predictions from models trained on different subsets of the data.\\n\\n**Overall, the document provides a comprehensive overview of the challenges and opportunities associated with high-quality human data in deep learning. It highlights the importance of careful task design, rater selection and training, and the use of various techniques for evaluating and improving data quality.** \\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add\n",
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ],
      "metadata": {
        "id": "ORkmxiDWpKLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrive documents"
      ],
      "metadata": {
        "id": "knvYSb8Isvj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Get similarity search\n",
        "\n",
        "query = \"Memory in agents\"\n",
        "sub_docs = vectorstore.similarity_search(query,k=1)\n",
        "sub_docs[0]\n"
      ],
      "metadata": {
        "id": "I_OvfvpYsuuN",
        "outputId": "bc144730-5b3b-4e49-c733-f7200cfff120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'doc_id': 'c97f0a47-9843-44e2-af16-82ee4f19f65c'}, page_content='This document explores the concept of LLM-powered autonomous agents, which are systems using large language models (LLMs) as their core controllers. It breaks down the key components of such agents: planning, memory, and tool use.\\n\\n**Planning** involves breaking down complex tasks into smaller subgoals and reflecting on past actions to improve future performance. Techniques like Chain of Thought (CoT), Tree of Thoughts, and ReAct are discussed for task decomposition and self-reflection.\\n\\n**Memory** encompasses short-term memory (in-context learning) and long-term memory (external vector stores). The document delves into the concept of Maximum Inner Product Search (MIPS) and various algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN for fast retrieval from vector stores.\\n\\n**Tool Use** equips LLMs with the ability to interact with external APIs and tools. Examples like MRKL, TALM, Toolformer, ChatGPT Plugins, OpenAI API function calling, and HuggingGPT are presented, showcasing how LLMs can leverage external resources to enhance their capabilities.\\n\\nThe document also discusses case studies like ChemCrow (a scientific discovery agent) and Generative Agents (a simulation of virtual characters interacting in a sandbox environment). It highlights proof-of-concept examples like AutoGPT and GPT-Engineer, which demonstrate the potential of LLMs for automating tasks and generating code.\\n\\nFinally, the document acknowledges challenges associated with LLM-powered agents, including finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes by emphasizing the ongoing research and development in this field, aiming to overcome these limitations and unlock the full potential of autonomous agents powered by LLMs.\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Get complete document\n",
        "\n",
        "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
        "retrieved_docs[0].page_content[0:500]\n"
      ],
      "metadata": {
        "id": "1SZJ4xegs5dy",
        "outputId": "719710fb-13a7-4944-a213-9953e5031d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAPTOR"
      ],
      "metadata": {
        "id": "OgWlzFNOtpA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/praffuln/generative-ai/blob/master/rag/rag-1/RAPTOR.ipynb#scrollTo=arHfZItPk-fr"
      ],
      "metadata": {
        "id": "OpixCFLbx3Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ColBERT"
      ],
      "metadata": {
        "id": "mB7gOwztx6pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAGatouille makes it as simple to use ColBERT.\n",
        "\n",
        "ColBERT generates a contextually influenced vector for each token in the passages.\n",
        "\n",
        "ColBERT similarly generates vectors for each token in the query.\n",
        "\n",
        "Then, the score of each document is the sum of the maximum similarity of each query embedding to any of the document embeddings:\n",
        "\n",
        "See here and here and here.\n",
        "\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2024/04/colbert-improve-retrieval-performance-with-token-level-vector-embeddings/\n",
        "\n"
      ],
      "metadata": {
        "id": "WjmomSs8yD0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U ragatouille\n"
      ],
      "metadata": {
        "id": "oDNbH4IJAULv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n"
      ],
      "metadata": {
        "id": "Xhl6o95GAaqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_page(title: str):\n",
        "    \"\"\"\n",
        "    Retrieve the full text content of a Wikipedia page.\n",
        "\n",
        "    :param title: str - Title of the Wikipedia page.\n",
        "    :return: str - Full text content of the page as raw string.\n",
        "    \"\"\"\n",
        "    # Wikipedia API endpoint\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Parameters for the API request\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "    }\n",
        "\n",
        "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
        "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
        "\n",
        "    response = requests.get(URL, params=params, headers=headers)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extracting page content\n",
        "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "    return page[\"extract\"] if \"extract\" in page else None\n",
        "\n",
        "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n"
      ],
      "metadata": {
        "id": "Ib4GqX2XDtrh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAG.index(\n",
        "    collection=[full_document],\n",
        "    index_name=\"Miyazaki-123\",\n",
        "    max_document_length=180,\n",
        "    split_documents=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ms2HhLv1EFNA",
        "outputId": "4fe3c3f7-1a0b-46ad-fb24-44120ddaa527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
            "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
            "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
            "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
            "--------------------\n",
            "\n",
            "\n",
            "[Aug 20, 09:13:52] #> Creating directory .ragatouille/colbert/indexes/Miyazaki-123 \n",
            "\n",
            "\n",
            "[Aug 20, 09:13:53] [0] \t\t #> Encoding 79 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "100%|██████████| 3/3 [00:54<00:00, 18.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:14:48] [0] \t\t avg_doclen_est = 132.35443115234375 \t len(local_sample) = 79\n",
            "[Aug 20, 09:14:48] [0] \t\t Creating 1,024 partitions.\n",
            "[Aug 20, 09:14:48] [0] \t\t *Estimated* 10,456 embeddings.\n",
            "[Aug 20, 09:14:48] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Miyazaki-123/plan.json ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: number of training points (9934) is less than the minimum recommended (10240)\n",
            "used 18 iterations (4.2399s) to cluster 9934 items into 1024 clusters\n",
            "[0.036, 0.042, 0.039, 0.036, 0.032, 0.035, 0.033, 0.038, 0.035, 0.034, 0.034, 0.035, 0.035, 0.037, 0.038, 0.038, 0.033, 0.035, 0.035, 0.04, 0.038, 0.036, 0.032, 0.038, 0.038, 0.032, 0.035, 0.034, 0.037, 0.035, 0.038, 0.039, 0.037, 0.036, 0.033, 0.032, 0.036, 0.035, 0.034, 0.04, 0.034, 0.04, 0.034, 0.032, 0.037, 0.032, 0.034, 0.036, 0.036, 0.031, 0.032, 0.034, 0.032, 0.036, 0.034, 0.036, 0.041, 0.037, 0.036, 0.031, 0.034, 0.034, 0.035, 0.033, 0.036, 0.035, 0.037, 0.036, 0.032, 0.034, 0.036, 0.033, 0.034, 0.035, 0.037, 0.033, 0.033, 0.037, 0.037, 0.034, 0.034, 0.04, 0.033, 0.038, 0.034, 0.036, 0.036, 0.037, 0.034, 0.04, 0.035, 0.036, 0.035, 0.039, 0.035, 0.033, 0.038, 0.037, 0.037, 0.038, 0.039, 0.04, 0.036, 0.036, 0.036, 0.034, 0.037, 0.033, 0.035, 0.031, 0.034, 0.037, 0.038, 0.032, 0.037, 0.034, 0.036, 0.037, 0.035, 0.037, 0.035, 0.035, 0.033, 0.035, 0.034, 0.036, 0.036, 0.036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:14:52] [0] \t\t #> Encoding 79 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|███▎      | 1/3 [00:19<00:39, 19.77s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [00:41<00:20, 20.90s/it]\u001b[A\n",
            "100%|██████████| 3/3 [00:50<00:00, 16.87s/it]\n",
            "1it [00:50, 50.96s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 688.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:15:43] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Aug 20, 09:15:43] #> Building the emb2pid mapping..\n",
            "[Aug 20, 09:15:43] len(emb2pid) = 10456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1024/1024 [00:00<00:00, 21846.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:15:43] #> Saved optimized IVF to .ragatouille/colbert/indexes/Miyazaki-123/ivf.pid.pt\n",
            "Done indexing!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.ragatouille/colbert/indexes/Miyazaki-123'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
        "results\n"
      ],
      "metadata": {
        "id": "sbEAC0O6EGk-",
        "outputId": "f0a1a4e9-a3b6-4e36-cbc9-e12441e93c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading searcher for index Miyazaki-123 for the first time... This may take a few seconds\n",
            "[Aug 20, 09:17:33] #> Loading codec...\n",
            "[Aug 20, 09:17:33] #> Loading IVF...\n",
            "[Aug 20, 09:17:33] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Aug 20, 09:18:10] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3429.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:18:10] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 271.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:18:10] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Aug 20, 09:18:44] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "Searcher loaded!\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . What animation studio did Miyazaki found?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  7284,  2996,  2106,  2771,  3148, 18637,  2179,\n",
            "         1029,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103])\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, Japanese: [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City in the Empire of Japan, Miyazaki expressed interest in manga and animation from an early age, and he joined Toei Animation in 1963. During his early years at Toei Animation he worked as an in-between artist and later collaborated with director Isao Takahata.',\n",
              "  'score': 25.629735946655273,\n",
              "  'rank': 1,\n",
              "  'document_id': 'e2ae4a3f-2225-4e0c-9e6c-e7fc71fc6dfa',\n",
              "  'passage_id': 0},\n",
              " {'content': \"In April 1984, Miyazaki opened his own office in Suginami Ward, naming it Nibariki.\\n\\n\\n=== Studio Ghibli ===\\n\\n\\n==== Early films (1985–1996) ====\\nOn June 15, 1985, Miyazaki and Takahata founded the animation production company Studio Ghibli as a subsidiary of Tokuma Shoten. Studio Ghibli's first film was Laputa: Castle in the Sky (1986), directed by Miyazaki. Some of the architecture in the film was also inspired by a Welsh mining town; Miyazaki witnessed the mining strike upon his first visit to Wales in 1984 and admired the miners' dedication to their work and community. Laputa was released on August 2, 1986, by the Toei Company.\",\n",
              "  'score': 25.573522567749023,\n",
              "  'rank': 2,\n",
              "  'document_id': 'e2ae4a3f-2225-4e0c-9e6c-e7fc71fc6dfa',\n",
              "  'passage_id': 29},\n",
              " {'content': \"By this time, Miyazaki had moved to the animation studio Topcraft and was finding some of the staff to be unreliable. He eventually decided to bring on several of his previous collaborators for the film's production, including Takahata who would serve as producer. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with. Takahata enlisted experimental and minimalist musician Joe Hisaishi to compose the film's score. Nausicaä of the Valley of the Wind was released on March 11, 1984. It grossed ¥1.48 billion at the box office, and made an additional ¥742 million in distribution income. It is often seen as Miyazaki's pivotal work, cementing his reputation as an animator.\",\n",
              "  'score': 25.042720794677734,\n",
              "  'rank': 3,\n",
              "  'document_id': 'e2ae4a3f-2225-4e0c-9e6c-e7fc71fc6dfa',\n",
              "  'passage_id': 27}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = RAG.as_langchain_retriever(k=3)\n",
        "retriever.invoke(\"What animation studio did Miyazaki found?\")\n"
      ],
      "metadata": {
        "id": "hptJRtE-EMKn",
        "outputId": "080c3bfc-9d97-4878-e201-91ae7130f08a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, Japanese: [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City in the Empire of Japan, Miyazaki expressed interest in manga and animation from an early age, and he joined Toei Animation in 1963. During his early years at Toei Animation he worked as an in-between artist and later collaborated with director Isao Takahata.'),\n",
              " Document(page_content=\"In April 1984, Miyazaki opened his own office in Suginami Ward, naming it Nibariki.\\n\\n\\n=== Studio Ghibli ===\\n\\n\\n==== Early films (1985–1996) ====\\nOn June 15, 1985, Miyazaki and Takahata founded the animation production company Studio Ghibli as a subsidiary of Tokuma Shoten. Studio Ghibli's first film was Laputa: Castle in the Sky (1986), directed by Miyazaki. Some of the architecture in the film was also inspired by a Welsh mining town; Miyazaki witnessed the mining strike upon his first visit to Wales in 1984 and admired the miners' dedication to their work and community. Laputa was released on August 2, 1986, by the Toei Company.\"),\n",
              " Document(page_content=\"By this time, Miyazaki had moved to the animation studio Topcraft and was finding some of the staff to be unreliable. He eventually decided to bring on several of his previous collaborators for the film's production, including Takahata who would serve as producer. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with. Takahata enlisted experimental and minimalist musician Joe Hisaishi to compose the film's score. Nausicaä of the Valley of the Wind was released on March 11, 1984. It grossed ¥1.48 billion at the box office, and made an additional ¥742 million in distribution income. It is often seen as Miyazaki's pivotal work, cementing his reputation as an animator.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}