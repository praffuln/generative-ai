{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVK6jQk3O7S3tK+dERkTUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praffuln/generative-ai/blob/master/rag/rag-1/rag_input_patterns_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJK7SLf6nJkr",
        "outputId": "5b2fa849-12de-4ee2-f503-425b5e21375d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world!!\n"
          ]
        }
      ],
      "source": [
        "print('hello world!!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit google-generativeai python-dotenv langchain PyPDF2 chromadb faiss-cpu langchain_google_genai langchain-community chromadb langchain_community tiktoken langchain-openai langchainhub"
      ],
      "metadata": {
        "id": "RQ2kachBn-dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Keys For GOOGLE_API_KEY AND LANGSMITH"
      ],
      "metadata": {
        "id": "YMnvGXWZhrDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning value to variable\n",
        "GOOGLE_API_KEY=''\n",
        "LANGCHAIN_API_KEY = ''\n"
      ],
      "metadata": {
        "id": "dvFOprifTZNj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "ALaWrOJvc3Qe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "NSYmU7-pb0wM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_store(text_chunks):\n",
        "    # Create embeddings using a Google Generative AI model\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Create a vector store using FAISS from the provided text chunks and embeddings\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "\n",
        "    # Save the vector store locally with the name \"faiss_index\"\n",
        "    vector_store.save_local(\"faiss_index\")"
      ],
      "metadata": {
        "id": "6FU9cwhlcLdS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "# Create embeddings using a Google Generative AI model\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "udu-ZhVNcFzw",
        "outputId": "0d34ae9d-cfb4-419d-ad94-95c47ebc28ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps. This can be done using a variety of methods, including prompting the LLM with simple instructions, using task-specific instructions, or with human inputs. Task decomposition can help to improve model performance on complex tasks and shed light into the model's thinking process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}